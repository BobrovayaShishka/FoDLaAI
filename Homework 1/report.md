# Отчёт по заданию 1: Создание и манипуляции с тензорами

## 1.1 Создание тензоров

Реализовано создание 4-х типов тензоров:

- Тензор 3x4 со случайными значениями (0-1): `torch.rand(3, 4)`
- Тензор 2x3x4 с нулями: `torch.zeros(2, 3, 4)`
- Тензор 5x5 с единицами: `torch.ones(5, 5)`
- Тензор 4x4 с числами 0-15: `torch.arange(16).reshape(4, 4)`

**Вывод программы:**
```
Тензор 1:
tensor([[0.1541, 0.7739, 0.3014, 0.6545],
        [0.5785, 0.7131, 0.9050, 0.7485],
        [0.7349, 0.5758, 0.1634, 0.6243]])

Тензор 2:
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])

Тензор 3:
tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])

Тензор 4:
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])
```

## 1.2 Операции с тензорами

Для матриц A(3x4) и B(4x3) выполнены операции:
1. Транспонирование A: `A.T`
2. Матричное умножение: `torch.matmul(A, B)`
3. Поэлементное умножение: `A * B.T`
4. Сумма элементов A: `A.sum()`

**Вывод программы:**
```
A:
tensor([[0.2344, 0.4361, 0.9739, 0.1357],
        [0.9943, 0.8960, 0.7850, 0.1864],
        [0.2550, 0.9733, 0.7532, 0.4641]])

B:
tensor([[0.9111, 0.1252, 0.3224],
        [0.1579, 0.5847, 0.6895],
        [0.6331, 0.4186, 0.5698],
        [0.5078, 0.8355, 0.3521]])

Транспонированная A:
tensor([[0.2344, 0.9943, 0.2550],
        [0.4361, 0.8960, 0.9733],
        [0.9739, 0.7850, 0.7532],
        [0.1357, 0.1864, 0.4641]])

Матричное умножение:
tensor([[0.9679, 0.8054, 0.9789],
        [1.6390, 1.1327, 1.4512],
        [1.0985, 1.3040, 1.3459]])

Поэлементное умножение:
tensor([[0.2135, 0.0688, 0.6166, 0.0689],
        [0.1245, 0.5239, 0.3286, 0.1557],
        [0.0822, 0.6711, 0.4291, 0.1634]])

Сумма элементов A: tensor(7.0873)
```

## 1.3 Индексация и срезы

Для тензора 5x5x5 выполнены операции:
1. Первая строка: `tensor[0, :, :]`
2. Последний столбец: `tensor[:, :, -1]`
3. Центральная подматрица 2x2: `tensor[2:4, 2:4, 2]`
4. Элементы с чётными индексами: `tensor[::2, ::2, ::2]`

**Вывод программы:**
```
Первая строка:
tensor([[0.1397, 0.4596, 0.5385, 0.5178, 0.9604],
        [0.2493, 0.6878, 0.2448, 0.9690, 0.6141],
        [0.4332, 0.0327, 0.8056, 0.2659, 0.1281],
        [0.3295, 0.8745, 0.3445, 0.9224, 0.6491],
        [0.7859, 0.9091, 0.3439, 0.0166, 0.7049]])

Последний столбец:
tensor([[0.9604, 0.6141, 0.1281, 0.6491, 0.7049],
        [0.2304, 0.2346, 0.2972, 0.1692, 0.0429],
        [0.0444, 0.2594, 0.2095, 0.2489, 0.9598],
        [0.2050, 0.6279, 0.1102, 0.2121, 0.2829],
        [0.3216, 0.7184, 0.4840, 0.4343, 0.4432]])

Центральная подматрица:
tensor([[0.2298, 0.4296],
        [0.8570, 0.9898]])

Элементы с четными индексами:
tensor([[[0.1397, 0.5385, 0.9604],
         [0.4332, 0.8056, 0.1281],
         [0.7859, 0.3439, 0.7049]],

        [[0.8510, 0.0397, 0.0444],
         [0.3582, 0.2298, 0.2095],
         [0.6275, 0.1250, 0.9598]],

        [[0.5093, 0.4355, 0.3216],
         [0.0966, 0.0555, 0.4840],
         [0.1043, 0.3484, 0.4432]]])
```

## 1.4 Работа с формами

Тензор из 24 элементов преобразован в:
- 2x12, 3x8, 4x6
- 2x3x4, 2x2x2x3

**Вывод программы:**
```
Форма 1 (2x12):
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])

Форма 2 (3x8):
tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],
        [ 8,  9, 10, 11, 12, 13, 14, 15],
        [16, 17, 18, 19, 20, 21, 22, 23]])

Форма 3 (4x6):
tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11],
        [12, 13, 14, 15, 16, 17],
        [18, 19, 20, 21, 22, 23]])

Форма 4 (2x3x4):
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])

Форма 5 (2x2x2x3):
tensor([[[[ 0,  1,  2],
          [ 3,  4,  5]],

         [[ 6,  7,  8],
          [ 9, 10, 11]]],

        [[[12, 13, 14],
          [15, 16, 17]],

         [[18, 19, 20],
          [21, 22, 23]]]])
```

## Итог

Все функции успешно реализованы:
- Создание тензоров разных типов
- Операции с матрицами (транспонирование, умножение)
- Индексация многомерных тензоров
- Изменение формы тензора

Тесты пройдены, программа работает корректно.

# Отчёт по заданию 2: Автоматическое дифференцирование

## 2.1 Простые вычисления с градиентами

**Задача:**  
Реализована функция f(x,y,z) = x² + y² + z² + 2*x*y*z с вычислением градиентов:
- Созданы тензоры x, y, z с `requires_grad=True`
- Вычислена функция и её градиенты с помощью `backward()`
- Проведена аналитическая проверка градиентов:
  - df/dx = 2x + 2yz
  - df/dy = 2y + 2xz
  - df/dz = 2z + 2xy

**Ключевые части кода:**
```python
x = torch.tensor(x_val, dtype=torch.float32, requires_grad=True)
y = torch.tensor(y_val, dtype=torch.float32, requires_grad=True)
z = torch.tensor(z_val, dtype=torch.float32, requires_grad=True)

f = x**2 + y**2 + z**2 + 2*x*y*z
f.backward()
```

**Вывод программы:**
```
f(1.0, 2.0, 3.0) = 26.0
df/dx = 14.0, df/dy = 10.0, df/dz = 10.0
```

**Аналитическая проверка:**
```
df/dx = 2*1.0 + 2*2.0*3.0 = 2 + 12 = 14.0 ✓
df/dy = 2*2.0 + 2*1.0*3.0 = 4 + 6 = 10.0 ✓
df/dz = 2*3.0 + 2*1.0*2.0 = 6 + 4 = 10.0 ✓
```

---

## 2.2 Градиент функции потерь

**Задача:**  
Реализована функция MSE (Mean Squared Error) для линейной модели:
- Формула: MSE = (1/n) * Σ(y_pred - y_true)²
- Линейная модель: y_pred = w * x + b
- Вычислены градиенты по параметрам w и b:
  - dMSE/dw = (2/n) * Σ(y_pred - y_true) * x
  - dMSE/db = (2/n) * Σ(y_pred - y_true)

**Ключевые части кода:**
```python
w = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

y_pred = w * x + b
loss = torch.mean((y_pred - y_true)**2)
loss.backward()
```

**Вывод программы:**
```
MSE = 1.5419998168945312
grad w = -1.5099999904632568, grad b = -2.0399999618530273
```

**Параметры модели:**
```
w = 2.0, b = 1.0
x = [0.0, 0.25, 0.5, 0.75, 1.0]
y_true = [1.1, 1.9, 3.2, 3.8, 5.1]
```

**Проверка расчета:**
```
y_pred = [1.0, 1.5, 2.0, 2.5, 3.0]

dMSE/dw = (2/5) * (0.1*0.0 + 0.4*0.25 + 1.2*0.5 + 1.3*0.75 + 2.1*1.0) ≈ -1.51 ✓
dMSE/db = (2/5) * (0.1 + 0.4 + 1.2 + 1.3 + 2.1) ≈ -2.04 ✓
```

---

## 2.3 Цепное правило

**Задача:**  
Реализована составная функция f(x) = sin(x² + 1):
- Вычислен градиент df/dx с помощью `backward()`
- Проверка с помощью `torch.autograd.grad()`
- Аналитическая проверка: 
  - df/dx = 2x * cos(x² + 1)

**Ключевые части кода:**
```python
# Способ 1: backward()
x1 = torch.tensor(x_val, requires_grad=True)
f1 = torch.sin(x1**2 + 1)
f1.backward()
grad1 = x1.grad.item()

# Способ 2: autograd.grad()
x2 = torch.tensor(x_val, requires_grad=True)
f2 = torch.sin(x2**2 + 1)
grad2 = torch.autograd.grad(f2, x2)[0].item()
```

**Вывод программы:**
```
f(0.5) = sin(0.5^2 + 1) = 0.9489846229553223
df/dx = 0.3153223693370819
```

**Аналитическая проверка:**
```
df/dx = 2 * 0.5 * cos(0.25 + 1) = 1 * cos(1.25) ≈ 1 * 0.3153 = 0.3153 ✓
```

---

## Итог

**Результаты выполнения задания:**
1. Успешно реализовано автоматическое дифференцирование для многопараметрической функции
2. Рассчитаны градиенты для функции потерь MSE в линейной модели
3. Применено цепное правило для составной функции
4. Все результаты подтверждены аналитическими вычислениями

# Отчёт по заданию 3: Сравнение производительности CPU vs CUDA

## 3.1 Подготовка данных

Созданы три больших тензора:
1. Размер: `64 x 1024 x 1024` (64 матрицы 1024×1024)
2. Размер: `128 x 512 x 512` (128 матриц 512×512)
3. Размер: `256 x 256 x 256` (256 матриц 256×256)

Все тензоры заполнены случайными числами с помощью `torch.randn()`. Для измерений созданы копии тензоров на CPU и GPU.

## 3.2 Функция измерения времени

Реализована функция `measure_time()` для точного измерения времени выполнения операций:
- Для CPU: используется `time.perf_counter()`
- Для GPU: используется `torch.cuda.Event()` с синхронизацией
- Каждая операция выполняется 10 раз для получения среднего значения
- Перед измерениями выполняется "прогрев" для компиляции кода

## 3.3 Сравнение операций

Измерено время выполнения пяти операций на CPU и GPU:

| Операция               | CPU (мс) | GPU (мс) | Ускорение |
|------------------------|----------|----------|-----------|
| Матричное умножение    | 673.27   | 15.61    | 43.1x     |
| Поэлементное сложение  | 98.53    | 1.87     | 52.7x     |
| Поэлементное умножение | 102.42   | 1.87     | 54.7x     |
| Транспонирование       | 206.40   | 3.61     | 57.2x     |
| Сумма всех элементов   | 12.90    | 0.59     | 21.9x     |

**Ключевые наблюдения:**
1. Все операции значительно быстрее выполняются на GPU
2. Наибольшее абсолютное ускорение у матричного умножения (657.66 мс)
3. Наибольшее относительное ускорение у транспонирования (57.2x)

## 3.4 Анализ результатов

### Какие операции получают наибольшее ускорение на GPU?
Наибольшее ускорение наблюдается у операций с высокой степенью параллелизма:
- **Транспонирование (57.2x)**: Идеально параллелизуемая операция, где каждый элемент может обрабатываться независимо
- **Поэлементные операции (52.7-54.7x)**: Сложение и умножение также легко параллелизуются
- **Матричное умножение (43.1x)**: Хотя требует сложных вычислений, эффективно оптимизировано для GPU

### Почему некоторые операции могут быть медленнее на GPU?
В данном тесте все операции быстрее на GPU, но в целом операции могут быть медленнее при:
1. **Маленьких размерах данных**: Нагрузка не компенсирует затраты на передачу данных и запуск ядер
2. **Неподходящих операциях**: Последовательные операции
3. **Ограничениях памяти**
4. **Проблемах с выравниванием данных**: Неоптимальные шаблоны доступа к памяти

### Как размер матриц влияет на ускорение?
Размер матриц критически важен для ускорения:
1. **Большие матрицы** (1024×1024): 
   - Максимальное использование вычислительных блоков GPU
   - Эффективное скрытие задержек памяти
   - Наибольшее ускорение (до 57x)
   
2. **Средние матрицы** (512×512):
   - Хорошее ускорение, но не полностью использующее потенциал GPU
   
3. **Малые матрицы** (256×256):
   - Меньшее ускорение из-за накладных расходов
   - Оптимизированные алгоритмы для больших матриц менее эффективны

### Что происходит при передаче данных между CPU и GPU?
Передача данных - критически важный аспект:
1. **Значительные задержки**: Копирование больших тензоров между устройствами может занимать миллисекунды
2. **Стратегии оптимизации**:
   - Минимизация передач данных
   - Конвейеризация вычислений и передач
3. **В тесте**: Данные копировались один раз перед измерениями, поэтому затраты на передачу не влияли на результаты

## Итог
1. GPU обеспечивает значительное ускорение для матричных операций
2. Наибольшая выгода достигается для больших данных и легко параллелизуемых операций
3. Ключевой фактор эффективности - минимизация передачи данных между CPU и GPU
4. Для максимального ускорения необходимо:
   - Использовать большие тензоры
   - Выбирать подходящие операции
   - Оптимизировать шаблоны доступа к памяти
   - Минимизировать передачу данных между устройствами
