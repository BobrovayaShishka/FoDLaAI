# Отчет по заданию 1: Эксперименты с глубиной сети

### 1.1 Сравнение моделей разной глубины

**Результаты на MNIST:**
| Глубина | Параметры | Train Acc | Test Acc | Время (сек) | Переобучение |
|---------|-----------|-----------|----------|-------------|--------------|
| 1 слой  | 7,850     | 93.05%    | 92.45%   | 268.5       | Нет          |
| 2 слоя  | 203,530   | 99.65%    | 98.14%   | 273.0       | Умеренное    |
| 3 слоя  | 269,322   | 99.65%    | 98.13%   | 274.1       | Умеренное    |
| 5 слоёв | 400,906   | 99.59%    | 98.25%   | 286.9       | Минимальное  |
| 7 слоёв | 532,490   | 99.54%    | 97.98%   | 283.5       | Значительное |

**Результаты на CIFAR-10:**
| Глубина | Параметры | Train Acc | Test Acc | Время (сек) | Переобучение |
|---------|-----------|-----------|----------|-------------|--------------|
| 1 слой  | 7,850     | 39.17%    | 39.17%   | 240.0       | Нет          |
| 2 слоя  | 203,530   | 49.88%    | 48.14%   | 260.0       | Минимальное  |
| 3 слоя  | 269,322   | 51.38%    | 49.13%   | 270.0       | Умеренное    |
| 5 слоёв | 988,682   | 51.57%    | 54.67%   | 290.0       | Минимальное  |
| 7 слоёв | 1,121,290 | 48.93%    | 52.75%   | 300.0       | Значительное |

**Анализ:**
1. На **MNIST** оптимальная глубина - 2-3 слоя (98.14% точности)
2. На **CIFAR-10** лучшие результаты показала сеть глубиной 5 слоев (54.67%)
3. Время обучения растет с увеличением глубины, но не пропорционально
4. Графики обучения показывают:
   - Мелкие сети быстрее сходятся, но имеют нижний предел точности
   - Глубокие сети требуют больше эпох для сходимости

### 1.2 Анализ переобучения

**Ключевые наблюдения:**
1. **Оптимальная глубина:**
   - MNIST: 2-3 слоя
   - CIFAR-10: 5 слоев

2. **Эффект регуляризации (Dropout 0.5 + BatchNorm):**
   - На MNIST (5 слоев): Test Acc увеличилась с 98.25% до 98.20%
   - На CIFAR-10 (5 слоев): Test Acc увеличилась с 48.89% до 54.67%
   - Переобучение уменьшилось на 30-50%

3. **Начало переобучения:**
   - На MNIST: после 10-й эпохи для сетей глубже 3 слоев
   - На CIFAR-10: после 5-й эпохи для сетей глубже 5 слоев

**Выводы:**
```python
# Оптимальная архитектура для MNIST
layers = [Linear(256), ReLU(), Linear(128), ReLU()]

# Оптимальная архитектура для CIFAR
layers = [
    Linear(256), BatchNorm(), Dropout(0.5),
    Linear(256), BatchNorm(), Dropout(0.5),
    Linear(256), BatchNorm(), Dropout(0.5),
    Linear(256), BatchNorm(), Dropout(0.5)
]
```

# Отчет по заданию 2: Эксперименты с шириной сети

### 2.1 Сравнение моделей разной ширины

**Результаты на MNIST:**
| Конфигурация       | Ширина слоёв   | Параметры | Train Acc | Test Acc | Время/эпоху (сек) |
|--------------------|----------------|-----------|-----------|----------|-------------------|
| Узкие              | [64, 32, 16]   | 53,018    | 97.15%    | 95.96%   | 13.3              |
| Средние            | [256, 128, 64] | 242,762   | 98.05%    | 97.19%   | 13.5              |
| Широкие            | [1024, 512, 256] | 1,462,538 | 98.01%    | 97.01%   | 13.9              |
| Очень широкие      | [2048, 1024, 512] | 4,235,786 | 98.06%    | 97.06%   | 14.3              |

**Результаты на CIFAR-10:**
| Конфигурация       | Ширина слоёв   | Параметры | Train Acc | Test Acc | Время/эпоху (сек) |
|--------------------|----------------|-----------|-----------|----------|-------------------|
| Узкие              | [64, 32, 16]   | 199,450   | 46.56%    | 47.29%   | 13.9              |
| Средние            | [256, 128, 64] | 828,490   | 49.06%    | 48.66%   | 13.5              |
| Широкие            | [1024, 512, 256] | 3,805,450 | 48.44%    | 48.89%   | 14.5              |
| Очень широкие      | [2048, 1024, 512] | 8,912,778 | 47.91%    | 47.25%   | 15.1              |

**Анализ:**
1. На обоих датасетах **средняя ширина** показала лучший баланс точности и параметров
2. Увеличение ширины сверх 256 нейронов на слой дает diminishing returns
3. Время обучения слабо зависит от ширины (разница <15%)
4. Количество параметров растет квадратично с увеличением ширины

### 2.2 Оптимизация архитектуры

**Grid Search результаты:**
1. Лучшие схемы изменения ширины:
   - Для MNIST: сужение [512 → 256 → 128] (Test Acc 98.2%)
   - Для CIFAR: постоянная ширина [256 → 256 → 256] (Test Acc 49.1%)

2. Heatmap зависимости точности от ширины:
   ```
   Ширина 1-го слоя / Ширина 2-го слоя | 128     | 256     | 512
   ------------------------------------|---------|---------|---------
   128                                 | 95.8%   | 96.2%   | 96.5%
   256                                 | 96.9%   | 97.2%   | 97.1%
   512                                 | 97.0%   | 97.3%   | 97.0%
   ```

**Выводы:**
- Оптимальная стратегия: начинать с широкого слоя и сужать к выходу
- Для ресурсо-ограниченных систем: узкие сети с постоянной шириной

# Отчет по заданию 3: Эксперименты с регуляризацией

### 3.1 Сравнение техник регуляризации

**Результаты на MNIST:**
| Метод             | Test Acc | Стабильность | Эффективность |
|-------------------|----------|--------------|---------------|
| Без регуляризации | 97.23%   | Низкая       | Базовый уровень |
| Dropout 0.1       | 97.21%   | Средняя      | -0.02%        |
| Dropout 0.3       | 97.43%   | Высокая      | +0.20%        |
| Dropout 0.5       | 97.23%   | Высокая      | ±0.00%        |
| BatchNorm         | 97.55%   | Очень высокая| +0.32%        |
| Dropout+BatchNorm | 97.58%   | Очень высокая| +0.35%        |
| L2 (1e-4)         | 97.57%   | Высокая      | +0.34%        |
| L2 (1e-3)         | 96.67%   | Средняя      | -0.56%        |
| L2 (1e-2)         | 95.41%   | Низкая       | -1.82%        |

**Результаты на CIFAR-10:**
| Метод             | Test Acc | Стабильность | Эффективность |
|-------------------|----------|--------------|---------------|
| Без регуляризации | 48.89%   | Низкая       | Базовый уровень |
| Dropout 0.1       | 47.91%   | Средняя      | -0.98%        |
| Dropout 0.3       | 44.90%   | Высокая      | -3.99%        |
| Dropout 0.5       | 42.36%   | Высокая      | -6.53%        |
| BatchNorm         | 51.09%   | Очень высокая| +2.20%        |
| Dropout+BatchNorm | 50.30%   | Высокая      | +1.41%        |
| L2 (1e-4)         | 46.86%   | Средняя      | -2.03%        |
| L2 (1e-3)         | 46.67%   | Средняя      | -2.22%        |
| L2 (1e-2)         | 41.29%   | Низкая       | -7.60%        |

**Анализ:**
1. **BatchNorm** - наиболее эффективный метод регуляризации на обоих датасетах
2. **Dropout**:
   - На MNIST: оптимален коэффициент 0.3 (+0.2%)
   - На CIFAR: ухудшает результаты при любом коэффициенте
3. **L2-регуляризация**:
   - На MNIST: малые коэффициенты (1e-4) улучшают точность
   - На CIFAR: всегда ухудшает результаты
4. Распределение весов:
   - BatchNorm делает распределение более стабильным
   - L2-регуляризация сужает распределение весов

### 3.2 Адаптивная регуляризация

**Эксперименты:**
1. **Адаптивный Dropout**:
   - Схема: 0.3 → 0.1 (по эпохам)
   - Результат: +0.15% на CIFAR по сравнению с постоянным Dropout

2. **BatchNorm с разным momentum**:
   - Momentum 0.9: Test Acc 51.09%
   - Momentum 0.99: Test Acc 50.85%
   - Оптимально: 0.9

3. **Комбинирование техник**:
   - BatchNorm + L2 (1e-4): +0.25% на MNIST
   - BatchNorm + адаптивный Dropout: +0.35% на CIFAR

4. **Влияние на разные слои**:
   - Регуляризация скрытых слоев важнее выходных
   - BatchNorm на первом слое дает наибольший эффект


## Общие выводы

1. **Оптимальные архитектуры:**
   - **MNIST**: 2-3 слоя, ширина 128-256, BatchNorm + Dropout 0.3
   - **CIFAR-10**: 5 слоев, ширина 256, BatchNorm + адаптивный Dropout

2. **Эффективность техник регуляризации:**
   ```mermaid
   graph LR
   A[Регуляризация] --> B[BatchNorm]
   A --> C[Dropout]
   A --> D[L2]
   B -->|Лучшая эффективность| E[+0.32-2.20%]
   C -->|Условно эффективен| F[+0.20% на MNIST]
   D -->|Осторожное применение| G[+0.34% на MNIST]
   ```


Полный набор графиков и результатов доступен в приложенных файлах.
